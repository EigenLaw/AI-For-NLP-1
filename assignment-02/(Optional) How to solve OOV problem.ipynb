{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some words are not in our dictionary or corpus. When we using language model, we need to overcome this out-of-vocabulary(OOV) problems. There are so many intelligent man to solve this probelm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: How did you solve this problem in your programming task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: If the word don't in our dictionary, we treat the time which occurs as 1 in our programming task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Read about the 'Turing-Good Estimator', can explain the main points about this method, and may implement this method in your programming task\n",
    "\n",
    "Reference:\n",
    "* https://www.wikiwand.com/en/Good%E2%80%93Turing_frequency_estimation\n",
    "* https://github.com/Computing-Intelligence/References/blob/master/NLP/Natural-Language-Processing.pdf, Page-37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Turing-Good Estimator,the probability of the word which is seen r times is $P_r =  \\frac{(r+1)*N_{r+1}}{N*N_r}$.\n",
    "需要注意的是，在实际的自然语言处理中，一般会设置一个阈值T，仅对出现次数小于T的词做上述调整，一般取T=10。对于出现次数大于某一阈值的N-Gram使最大似然用频率计算是比较准确的。另外，因为实际语料的统计情况使得Nr+1<Nr不一定成立，Nr=0情况也可能出现，所以需要使用曲线拟合的方式替换掉原有的Nr，并使用Kartz退避公式计算$P_r$: $P_r = \\frac{(r+1)\\frac{N_{r+1}}{N_r}-r\\frac{(k+1)N_{k+1}}{N_1}}{N*(1-\\frac{(k+1)N_{k+1}}{N_1})}, 1 \\le r \\le k$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
